{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pyspark in c:\\users\\johnjesus\\appdata\\roaming\\python\\python311\\site-packages (3.5.2)\n",
      "Collecting boto3\n",
      "  Downloading boto3-1.37.2-py3-none-any.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in c:\\users\\johnjesus\\appdata\\roaming\\python\\python311\\site-packages (from pyspark) (0.10.9.7)\n",
      "Collecting botocore<1.38.0,>=1.37.2 (from boto3)\n",
      "  Downloading botocore-1.37.2-py3-none-any.whl.metadata (5.7 kB)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in c:\\users\\johnjesus\\appdata\\roaming\\python\\python311\\site-packages (from boto3) (1.0.1)\n",
      "Collecting s3transfer<0.12.0,>=0.11.0 (from boto3)\n",
      "  Downloading s3transfer-0.11.3-py3-none-any.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in c:\\users\\johnjesus\\appdata\\roaming\\python\\python311\\site-packages (from botocore<1.38.0,>=1.37.2->boto3) (2.9.0.post0)\n",
      "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in c:\\users\\johnjesus\\appdata\\roaming\\python\\python311\\site-packages (from botocore<1.38.0,>=1.37.2->boto3) (2.2.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\johnjesus\\appdata\\roaming\\python\\python311\\site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.38.0,>=1.37.2->boto3) (1.16.0)\n",
      "Downloading boto3-1.37.2-py3-none-any.whl (139 kB)\n",
      "Downloading botocore-1.37.2-py3-none-any.whl (13.3 MB)\n",
      "   ---------------------------------------- 0.0/13.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/13.3 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.3/13.3 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.3/13.3 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.5/13.3 MB 799.2 kB/s eta 0:00:17\n",
      "   -- ------------------------------------- 0.8/13.3 MB 838.9 kB/s eta 0:00:15\n",
      "   --- ------------------------------------ 1.0/13.3 MB 853.0 kB/s eta 0:00:15\n",
      "   --- ------------------------------------ 1.0/13.3 MB 853.0 kB/s eta 0:00:15\n",
      "   --- ------------------------------------ 1.3/13.3 MB 882.6 kB/s eta 0:00:14\n",
      "   ---- ----------------------------------- 1.6/13.3 MB 911.5 kB/s eta 0:00:13\n",
      "   ----- ---------------------------------- 1.8/13.3 MB 915.0 kB/s eta 0:00:13\n",
      "   ------ --------------------------------- 2.1/13.3 MB 931.9 kB/s eta 0:00:13\n",
      "   ------- -------------------------------- 2.4/13.3 MB 958.5 kB/s eta 0:00:12\n",
      "   ------- -------------------------------- 2.6/13.3 MB 974.1 kB/s eta 0:00:11\n",
      "   ------- -------------------------------- 2.6/13.3 MB 974.1 kB/s eta 0:00:11\n",
      "   --------- ------------------------------ 3.1/13.3 MB 1.0 MB/s eta 0:00:11\n",
      "   ---------- ----------------------------- 3.4/13.3 MB 1.0 MB/s eta 0:00:10\n",
      "   ----------- ---------------------------- 3.7/13.3 MB 1.0 MB/s eta 0:00:10\n",
      "   ----------- ---------------------------- 3.9/13.3 MB 1.1 MB/s eta 0:00:09\n",
      "   ------------ --------------------------- 4.2/13.3 MB 1.1 MB/s eta 0:00:09\n",
      "   ------------- -------------------------- 4.5/13.3 MB 1.1 MB/s eta 0:00:09\n",
      "   -------------- ------------------------- 4.7/13.3 MB 1.1 MB/s eta 0:00:08\n",
      "   -------------- ------------------------- 5.0/13.3 MB 1.1 MB/s eta 0:00:08\n",
      "   ---------------- ----------------------- 5.5/13.3 MB 1.2 MB/s eta 0:00:07\n",
      "   ----------------- ---------------------- 5.8/13.3 MB 1.2 MB/s eta 0:00:07\n",
      "   ------------------ --------------------- 6.0/13.3 MB 1.2 MB/s eta 0:00:07\n",
      "   ------------------- -------------------- 6.6/13.3 MB 1.2 MB/s eta 0:00:06\n",
      "   -------------------- ------------------- 6.8/13.3 MB 1.2 MB/s eta 0:00:06\n",
      "   ---------------------- ----------------- 7.3/13.3 MB 1.3 MB/s eta 0:00:05\n",
      "   ---------------------- ----------------- 7.6/13.3 MB 1.3 MB/s eta 0:00:05\n",
      "   ----------------------- ---------------- 7.9/13.3 MB 1.3 MB/s eta 0:00:05\n",
      "   ------------------------- -------------- 8.4/13.3 MB 1.3 MB/s eta 0:00:04\n",
      "   ------------------------- -------------- 8.7/13.3 MB 1.3 MB/s eta 0:00:04\n",
      "   --------------------------- ------------ 9.2/13.3 MB 1.3 MB/s eta 0:00:04\n",
      "   ---------------------------- ----------- 9.4/13.3 MB 1.3 MB/s eta 0:00:03\n",
      "   ----------------------------- ---------- 10.0/13.3 MB 1.4 MB/s eta 0:00:03\n",
      "   ------------------------------- -------- 10.5/13.3 MB 1.4 MB/s eta 0:00:03\n",
      "   -------------------------------- ------- 10.7/13.3 MB 1.4 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 11.3/13.3 MB 1.4 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 11.8/13.3 MB 1.5 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 12.3/13.3 MB 1.5 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 12.6/13.3 MB 1.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  13.1/13.3 MB 1.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  13.1/13.3 MB 1.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  13.1/13.3 MB 1.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  13.1/13.3 MB 1.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  13.1/13.3 MB 1.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  13.1/13.3 MB 1.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  13.1/13.3 MB 1.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 13.3/13.3 MB 1.3 MB/s eta 0:00:00\n",
      "Downloading s3transfer-0.11.3-py3-none-any.whl (84 kB)\n",
      "Installing collected packages: botocore, s3transfer, boto3\n",
      "  Attempting uninstall: botocore\n",
      "    Found existing installation: botocore 1.34.108\n",
      "    Uninstalling botocore-1.34.108:\n",
      "      Successfully uninstalled botocore-1.34.108\n",
      "  Attempting uninstall: s3transfer\n",
      "    Found existing installation: s3transfer 0.10.1\n",
      "    Uninstalling s3transfer-0.10.1:\n",
      "      Successfully uninstalled s3transfer-0.10.1\n",
      "Successfully installed boto3-1.37.2 botocore-1.37.2 s3transfer-0.11.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~umpy (C:\\Users\\JOHNJESUS\\AppData\\Roaming\\Python\\Python311\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~umpy (C:\\Users\\JOHNJESUS\\AppData\\Roaming\\Python\\Python311\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~umpy (C:\\Users\\JOHNJESUS\\AppData\\Roaming\\Python\\Python311\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~umpy (C:\\Users\\JOHNJESUS\\AppData\\Roaming\\Python\\Python311\\site-packages)\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "awscli 1.32.108 requires botocore==1.34.108, but you have botocore 1.37.2 which is incompatible.\n",
      "awscli 1.32.108 requires s3transfer<0.11.0,>=0.10.0, but you have s3transfer 0.11.3 which is incompatible.\n",
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "! pip install pyspark boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Titanic_Classification_Local\") \\\n",
    "    .config(\"spark.master\", \"local[*]\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "|PassengerId|Survived|Pclass|                Name|   Sex| Age|SibSp|Parch|          Ticket|   Fare|Cabin|Embarked|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "|          1|       0|     3|Braund, Mr. Owen ...|  male|22.0|    1|    0|       A/5 21171|   7.25| NULL|       S|\n",
      "|          2|       1|     1|Cumings, Mrs. Joh...|female|38.0|    1|    0|        PC 17599|71.2833|  C85|       C|\n",
      "|          3|       1|     3|Heikkinen, Miss. ...|female|26.0|    0|    0|STON/O2. 3101282|  7.925| NULL|       S|\n",
      "|          4|       1|     1|Futrelle, Mrs. Ja...|female|35.0|    1|    0|          113803|   53.1| C123|       S|\n",
      "|          5|       0|     3|Allen, Mr. Willia...|  male|35.0|    0|    0|          373450|   8.05| NULL|       S|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv(\"train.csv\", header=True, inferSchema=True)\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, when\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "\n",
    "# Selecting required columns\n",
    "df = df.select(\"Survived\", \"Pclass\", \"Sex\", \"Age\", \"SibSp\", \"Parch\", \"Fare\")\n",
    "\n",
    "# Convert 'Sex' column (Male=1, Female=0)\n",
    "df = df.withColumn(\"Sex\", when(col(\"Sex\") == \"male\", 1).otherwise(0))\n",
    "\n",
    "# Handle missing values\n",
    "df = df.fillna({\"Age\": df.selectExpr(\"avg(Age)\").collect()[0][0]})\n",
    "\n",
    "# Convert target column\n",
    "label_indexer = StringIndexer(inputCol=\"Survived\", outputCol=\"label\")\n",
    "\n",
    "# Assemble features\n",
    "feature_assembler = VectorAssembler(\n",
    "    inputCols=[\"Pclass\", \"Sex\", \"Age\", \"SibSp\", \"Parch\", \"Fare\"],\n",
    "    outputCol=\"features\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+---+----+-----+-----+-------+\n",
      "|Survived|Pclass|Sex| Age|SibSp|Parch|   Fare|\n",
      "+--------+------+---+----+-----+-----+-------+\n",
      "|       0|     3|  1|22.0|    1|    0|   7.25|\n",
      "|       1|     1|  0|38.0|    1|    0|71.2833|\n",
      "|       1|     3|  0|26.0|    0|    0|  7.925|\n",
      "|       1|     1|  0|35.0|    1|    0|   53.1|\n",
      "|       0|     3|  1|35.0|    0|    0|   8.05|\n",
      "+--------+------+---+----+-----+-----+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.pipeline import Pipeline\n",
    "\n",
    "# Split data\n",
    "train_df, test_df = df.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# Initialize model\n",
    "lr = LogisticRegression(featuresCol=\"features\", labelCol=\"label\")\n",
    "\n",
    "# Create pipeline\n",
    "pipeline = Pipeline(stages=[label_indexer, feature_assembler, lr])\n",
    "\n",
    "# Train model\n",
    "model = pipeline.fit(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy: 0.7974238875878221\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "predictions = model.transform(test_df)\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator(labelCol=\"label\", rawPredictionCol=\"prediction\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "\n",
    "print(f\"Model Accuracy: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+\n",
      "|        features_new|prediction|\n",
      "+--------------------+----------+\n",
      "|[3.0,1.0,22.0,1.0...|       0.0|\n",
      "|[1.0,0.0,38.0,1.0...|       1.0|\n",
      "+--------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "manual_data = spark.createDataFrame([\n",
    "    (3, 1, 22.0, 1, 0, 7.25),\n",
    "    (1, 0, 38.0, 1, 0, 71.28)\n",
    "], [\"Pclass\", \"Sex\", \"Age\", \"SibSp\", \"Parch\", \"Fare\"])\n",
    "\n",
    "\n",
    "# Modify the feature assembler to use a new column name\n",
    "feature_assembler = VectorAssembler(\n",
    "    inputCols=[\"Pclass\", \"Sex\", \"Age\", \"SibSp\", \"Parch\", \"Fare\"],\n",
    "    outputCol=\"features_new\"  # New output column\n",
    ")\n",
    "\n",
    "# Transform manual data\n",
    "manual_data = feature_assembler.transform(manual_data)\n",
    "\n",
    "# Run predictions\n",
    "manual_predictions = model.transform(manual_data)\n",
    "manual_predictions.select(\"features_new\", \"prediction\").show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to titanic_model_local\n"
     ]
    }
   ],
   "source": [
    "model_path = \"titanic_model_local\"\n",
    "model.write().overwrite().save(model_path)\n",
    "print(f\"Model saved to {model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import PipelineModel\n",
    "model = PipelineModel.load(\"./titanic_model_local\")  # ✅ Correct\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+\n",
      "|            features|prediction|\n",
      "+--------------------+----------+\n",
      "|[3.0,1.0,22.0,1.0...|       0.0|\n",
      "|[1.0,0.0,38.0,1.0...|       1.0|\n",
      "+--------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml import PipelineModel\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder.appName(\"Local_Titanic_Prediction\").getOrCreate()\n",
    "\n",
    "# Load the full PipelineModel (not just LogisticRegressionModel)\n",
    "model = PipelineModel.load(\"./titanic_model_local\")  # ✅ Correct\n",
    "\n",
    "# Create test data (Make sure column names match training)\n",
    "manual_data = spark.createDataFrame([\n",
    "    (3, 1, 22.0, 1, 0, 7.25),\n",
    "    (1, 0, 38.0, 1, 0, 71.28)\n",
    "], [\"Pclass\", \"Sex\", \"Age\", \"SibSp\", \"Parch\", \"Fare\"])\n",
    "\n",
    "# Run prediction using the loaded model\n",
    "manual_predictions = model.transform(manual_data)\n",
    "\n",
    "# Show predictions\n",
    "manual_predictions.select(\"features\", \"prediction\").show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
